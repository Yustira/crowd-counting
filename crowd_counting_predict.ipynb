{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crowd-counting-predict.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yustira/crowd-counting/blob/master/crowd_counting_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj1DVRXHNdE7",
        "colab_type": "text"
      },
      "source": [
        "# Crowd Counting Prediction\n",
        "This notebook is used for prediction only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP02M3SCN45y",
        "colab_type": "text"
      },
      "source": [
        "## Load the imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyxYaAkANO2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from tensorflow.random import set_seed\n",
        "set_seed(1)\n",
        "\n",
        "import scipy\n",
        "from scipy import spatial\n",
        "from scipy import ndimage\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import Conv2D, Average, MaxPooling2D, Flatten,concatenate, Input, Dense, UpSampling2D, Conv2DTranspose, ReLU, Activation, BatchNormalization,Lambda\n",
        "from keras.models import Model\n",
        "from keras.losses import mean_squared_error\n",
        "from keras.losses import mean_absolute_percentage_error\n",
        "from keras.optimizers import Adam, Nadam\n",
        "from keras.applications import vgg16\n",
        "import keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3qL57RsN-6a",
        "colab_type": "text"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mscbhvaOBBt",
        "colab_type": "text"
      },
      "source": [
        "### Run this code only when you are using Colab\n",
        "This code will download dataset needed to colab's temporary storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DLk-IDDhiSxH"
      },
      "source": [
        "#### Install PyDrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3YGcmtWpiSxg",
        "colab": {}
      },
      "source": [
        "!pip install PyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cGhZ9y0wiSx6"
      },
      "source": [
        "#### Google Drive Authentication"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rtWVN-3CiSx9",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OyoB2rYQiSyK"
      },
      "source": [
        "#### Download the file from the link"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llYyytErRbh1",
        "colab_type": "text"
      },
      "source": [
        "##### Download images file in `.npy` format from this [link](https://drive.google.com/file/d/1y0BoU7RIisMgj4PsCPbY02PPm4CEXaEF/view)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JjN8SHC7iSyN",
        "colab": {}
      },
      "source": [
        "IMAGE_FILE_ID = \"1y0BoU7RIisMgj4PsCPbY02PPm4CEXaEF\"\n",
        "downloaded = drive.CreateFile({'id':IMAGE_FILE_ID})\n",
        "downloaded.GetContentFile('images.npy')        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG8klbRYXNDR",
        "colab_type": "text"
      },
      "source": [
        "##### Download density map file that has been generated for images in the dataset from this [link](https://drive.google.com/file/d/1-1JXmD6sumzJcATFQzm9Yq7CCDZ2K7cT/view)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8VTBle7SiSye",
        "colab": {}
      },
      "source": [
        "IMAGE_DENSITY_FILE_ID = \"1-1JXmD6sumzJcATFQzm9Yq7CCDZ2K7cT\"\n",
        "downloaded = drive.CreateFile({'id':IMAGE_DENSITY_FILE_ID})\n",
        "downloaded.GetContentFile('images_density.npy') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1DfPEiHCBDX",
        "colab_type": "text"
      },
      "source": [
        "### Load dataset from file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0eO4RPT58on",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images=np.load('images.npy')\n",
        "images_density=np.load('images_density.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsfeTM-W58oy",
        "colab_type": "text"
      },
      "source": [
        "## Data Preview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESsst8u458o2",
        "colab_type": "text"
      },
      "source": [
        "### Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDPfR7KH58o5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img=plt.imshow(images[200])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6hg1LY858pS",
        "colab_type": "text"
      },
      "source": [
        "### Images + Density Map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvXjPzpC58pc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(images[200])\n",
        "plt.imshow(images_density[200],cmap='prism',interpolation='bicubic',alpha=0.25)\n",
        "print('number of people: %4.2f' %np.sum(images_density[200]/1000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztRMYsRG58ps",
        "colab_type": "text"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlRyobky58pu",
        "colab_type": "text"
      },
      "source": [
        "### Import VGG16 as Baseline\n",
        "Set the input shape for our model\\\n",
        "Creating baseline model with VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yoh2EwqJ58px",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "size=(96,128)\n",
        "input_shape=(size[0],size[1],3)\n",
        "baseline_vgg=vgg16.VGG16(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFCVAQRH58p4",
        "colab_type": "text"
      },
      "source": [
        "### Main Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX9cpsSf58p5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Crowd_CNN(input_shape=input_shape):\n",
        "    \n",
        "    input_flow=Input(input_shape)\n",
        "    base_filter=8\n",
        "    \n",
        "    x_vgg=baseline_vgg.get_layer(index=1)(input_flow)\n",
        "    for i in range(2,10):\n",
        "        x_vgg=baseline_vgg.get_layer(index=i)(x_vgg)\n",
        "        \n",
        "    \n",
        "    \n",
        "    x_1=Conv2D(base_filter*16,9,padding='same',activation='relu')(x_vgg)\n",
        "    x_1=BatchNormalization()(x_1)\n",
        "    x_1=Conv2D(base_filter*8,7,padding='same',activation='relu')(x_1)\n",
        "    x_1=BatchNormalization()(x_1)\n",
        "    x_1=Conv2D(base_filter*4,5,padding='same',activation='relu')(x_1)\n",
        "    x_1=BatchNormalization()(x_1)\n",
        "    x_1=Conv2D(1,1,padding='same',activation='relu')(x_1)\n",
        "    x_1=BatchNormalization()(x_1)\n",
        "    \n",
        "    \n",
        "    x_2=Conv2D(base_filter*8,7,padding='same',activation='relu')(x_vgg)\n",
        "    x_2=BatchNormalization()(x_2)\n",
        "    x_2=Conv2D(base_filter*4,5,padding='same',activation='relu')(x_2)\n",
        "    x_2=BatchNormalization()(x_2)\n",
        "    x_2=Conv2D(base_filter*2,3,padding='same',activation='relu')(x_2)\n",
        "    x_2=BatchNormalization()(x_2)\n",
        "    x_2=Conv2D(1,1,padding='same',activation='relu')(x_2)\n",
        "    x_2=BatchNormalization()(x_2)\n",
        "    \n",
        "    \n",
        "    x_3=Conv2D(base_filter*4,5,padding='same',activation='relu')(x_vgg)\n",
        "    x_3=BatchNormalization()(x_3)\n",
        "    x_3=Conv2D(base_filter*2,3,padding='same',activation='relu')(x_3)\n",
        "    x_3=BatchNormalization()(x_3)\n",
        "    x_3=Conv2D(base_filter*1,1,padding='same',activation='relu')(x_3)\n",
        "    x_3=BatchNormalization()(x_3)\n",
        "    x_3=Conv2D(1,1,padding='same',activation='relu')(x_3)\n",
        "    x_3=BatchNormalization()(x_3)\n",
        "    \n",
        "    x_4=Conv2D(base_filter*4,3,padding='same',activation='relu')(x_vgg)\n",
        "    x_4=BatchNormalization()(x_4)\n",
        "    x_4=Conv2D(base_filter*2,1,padding='same',activation='relu')(x_4)\n",
        "    x_4=BatchNormalization()(x_4)\n",
        "    x_4=Conv2D(base_filter*1,1,padding='same',activation='relu')(x_4)\n",
        "    x_4=BatchNormalization()(x_4)\n",
        "    x_4=Conv2D(1,1,padding='same',activation='relu')(x_4)\n",
        "    x_4=BatchNormalization()(x_4)\n",
        "    \n",
        "    \n",
        "    x_conct=concatenate([x_1,x_2,x_3,x_4])\n",
        "    x_fel=Flatten()(x_conct)\n",
        "    x_fel=Dense(base_filter*8,activation='relu')(x_fel)\n",
        "    x_fel=Dense(4,activation='softmax')(x_fel)\n",
        "    \n",
        "    x=Lambda(lambda x: x*(1+x_fel))(x_conct)\n",
        "\n",
        "    \n",
        "    x=Conv2D(base_filter*2,5,padding='same',activation='relu')(x)\n",
        "    x=BatchNormalization()(x)\n",
        "    x=concatenate([x,x_conct])\n",
        "    x=Conv2D(base_filter*2,3,padding='same',activation='relu')(x)\n",
        "    x=BatchNormalization()(x)\n",
        "    x=Conv2DTranspose(base_filter*2, kernel_size=(2, 2), strides=(2, 2))(x)\n",
        "    x=BatchNormalization()(x)\n",
        "    x=Conv2D(base_filter*2,3,padding='same',activation='relu')(x)\n",
        "    x=BatchNormalization()(x)\n",
        "    x=Conv2DTranspose(base_filter, kernel_size=(2, 2), strides=(2, 2))(x)\n",
        "    x=BatchNormalization()(x)\n",
        "    x=Conv2D(base_filter,1,padding='same',activation='relu')(x)\n",
        "    x=BatchNormalization()(x)\n",
        "    x=Conv2D(1,1,padding='same',activation='relu')(x)\n",
        "    \n",
        "    model=Model(inputs=input_flow,outputs=x)    \n",
        "    \n",
        "    return model\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72o-ri7g58qF",
        "colab_type": "text"
      },
      "source": [
        "### SSIM and Euclidean Loss + Custom MAE metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aHhi_6K58qG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ssim_loss(y_true, y_pred, c1=0.01**2, c2=0.03**2):\n",
        "    weights_initial = np.multiply(\n",
        "        cv2.getGaussianKernel(12, 1.5),\n",
        "        cv2.getGaussianKernel(16, 1.5).T\n",
        "    )\n",
        "    weights_initial = weights_initial.reshape(*weights_initial.shape, 1, 1)\n",
        "    weights_initial = K.cast(weights_initial, tf.float32)\n",
        "\n",
        "    mu_F = tf.nn.conv2d(y_pred, weights_initial, [1, 1, 1, 1], padding='SAME')\n",
        "    mu_Y = tf.nn.conv2d(y_true, weights_initial, [1, 1, 1, 1], padding='SAME')\n",
        "    mu_F_mu_Y = tf.multiply(mu_F, mu_Y)\n",
        "    mu_F_squared = tf.multiply(mu_F, mu_F)\n",
        "    mu_Y_squared = tf.multiply(mu_Y, mu_Y)\n",
        "\n",
        "    sigma_F_squared = tf.nn.conv2d(tf.multiply(y_pred, y_pred), weights_initial, [1, 1, 1, 1], padding='SAME') - mu_F_squared\n",
        "    sigma_Y_squared = tf.nn.conv2d(tf.multiply(y_true, y_true), weights_initial, [1, 1, 1, 1], padding='SAME') - mu_Y_squared\n",
        "    sigma_F_Y = tf.nn.conv2d(tf.multiply(y_pred, y_true), weights_initial, [1, 1, 1, 1], padding='SAME') - mu_F_mu_Y\n",
        "\n",
        "    ssim = ((2 * mu_F_mu_Y + c1) * (2 * sigma_F_Y + c2)) / ((mu_F_squared + mu_Y_squared + c1) * (sigma_F_squared + sigma_Y_squared + c2))\n",
        "\n",
        "    return 1 - tf.reduce_mean(ssim, axis=[1, 2, 3])\n",
        "\n",
        "\n",
        "def ssim_eucli_loss(y_true, y_pred, alpha=0.0025):\n",
        "    ssim = ssim_loss(y_true, y_pred)\n",
        "    eucli = mean_squared_error(y_true, y_pred)\n",
        "    loss = eucli + alpha * ssim\n",
        "    return loss\n",
        "\n",
        "def mae_cnt(labels,preds):\n",
        "    cnt_label=K.sum(labels)\n",
        "    cnt_pred=K.sum(preds)\n",
        "    return K.abs(cnt_label-cnt_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "witidxQT58qO",
        "colab_type": "text"
      },
      "source": [
        "### Define the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItuvhtPRQPcj",
        "colab_type": "text"
      },
      "source": [
        "#### Download the model weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wyfPpj2Gsr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://raw.githubusercontent.com/Yustira/crowd-counting/master/Crowd-CNN \\\n",
        "    -O Crowd-CNN.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwCr8whOb8E_",
        "colab_type": "text"
      },
      "source": [
        "#### Model declaration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwjVIIVd58qQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr=1e-5\n",
        "model=Crowd_CNN()\n",
        "model.load_weights(\"Crowd-CNN.h5\")\n",
        "Optimizer=Nadam(lr)\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUr8oFoBQYGR",
        "colab_type": "text"
      },
      "source": [
        "#### Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm-RCegW58qe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=Optimizer,loss=ssim_eucli_loss,metrics=[mae_cnt])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8EE9zN4A91N",
        "colab_type": "text"
      },
      "source": [
        "## Create test data and predict\n",
        "This section include pre-process data that is used for prediction\\\n",
        "We provide two ways of prediction, which is image from mall dataset or image that we can upload ourselves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiIQjDyXcNVX",
        "colab_type": "text"
      },
      "source": [
        "### Data from mall dataset\n",
        "data will be selected randomly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M4e5BGNcXDL",
        "colab_type": "text"
      },
      "source": [
        "#### Select random data\n",
        "Randomly selected data will be saved on variable X and y for input and label with size of `test_size`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCD9p9ZIjmyq",
        "colab_type": "text"
      },
      "source": [
        "test size is number of image that we want to predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGEaDWHgcyvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_size = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8T4ewtg8_7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_test = np.random.randint(images.shape[0], size=test_size)\n",
        "idx_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFRgn4btEU6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = images[idx_test]/255  # normalize\n",
        "y = images_density[idx_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZSOM6J9YwuA",
        "colab_type": "text"
      },
      "source": [
        "#### Selected Image Data Preview"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkURGyVcFH6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.title(\"Image for data 0\")\n",
        "plt.imshow(X[0])\n",
        "plt.imshow(y[0],cmap='prism',interpolation='bicubic',alpha=0.25)\n",
        "print('number of people: %4.2f' %np.sum(y[0]/1000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c9S4NdmQFmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(X[1])\n",
        "plt.imshow(y[1],cmap='prism',interpolation='bicubic',alpha=0.25)\n",
        "print('number of people: %4.2f' %np.sum(y[1]/1000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-3IpW5DFnJN",
        "colab_type": "text"
      },
      "source": [
        "#### Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE6Acu_qksKI",
        "colab_type": "text"
      },
      "source": [
        "##### Pre-process data for prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG4yqUIDCZuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = np.expand_dims(y,-1)\n",
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A06o_hzkX_x",
        "colab_type": "text"
      },
      "source": [
        "##### Function for predicting on input image(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51ciMLpGCWh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def full_eval(x,smooth=False):\n",
        "    y=np.zeros([480,640])\n",
        "    for i in range(0,480,size[0]):\n",
        "        for j in range(0,640,size[1]):\n",
        "            y[i:i+size[0],j:j+size[1]]+=model.predict(x[:,i:i+size[0],j:j+size[1],:])[0,:,:,0]\n",
        "    if smooth:\n",
        "        y=ndimage.filters.gaussian_filter(y, 2, mode='constant')\n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_FkuqMfkfWV",
        "colab_type": "text"
      },
      "source": [
        "##### Prediction for all input images (`X`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzW5cl8_bVAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0, test_size):\n",
        "\n",
        "  y_predict = full_eval(X[i:i+1],smooth=True)\n",
        "\n",
        "  print(\"Prediction for data {}\".format(i))\n",
        "  print('Predicted Number of people: %4.2f' %(np.sum(y_predict)/1000))\n",
        "  print('Exact Number of people: %4.2f' %(np.sum(y[i])/1000))\n",
        "  print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syXSHFGZkpM-",
        "colab_type": "text"
      },
      "source": [
        "##### Prediction per single image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPF0BAXeCjBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e=0\n",
        "y_predict=full_eval(X[e:e+1],smooth=True)\n",
        "plt.figure()\n",
        "f, axarr = plt.subplots(1,2,figsize=(20, 20)) \n",
        "\n",
        "# plot for predicted density map\n",
        "axarr[0].title.set_text('predicted density map')\n",
        "axarr[0].imshow(X[e,:,:,:])\n",
        "axarr[0].imshow(y_predict, alpha=0.7)\n",
        "\n",
        "# plot for actual density map\n",
        "axarr[1].title.set_text('actual density map')\n",
        "axarr[1].imshow(X[e,:,:,:])\n",
        "axarr[1].imshow(y[e,:,:,0])\n",
        "print('Predicted Number of people: %4.2f' %(np.sum(y_predict)/1000))\n",
        "print('Exact Number of people: %4.2f' %(np.sum(y[e])/1000))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4dCQLAraWwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e=1\n",
        "y_predict=full_eval(X[e:e+1],smooth=True)\n",
        "plt.figure()\n",
        "f, axarr = plt.subplots(1,2,figsize=(20, 20)) \n",
        "\n",
        "# plot for predicted density map\n",
        "axarr[0].title.set_text('predicted density map')\n",
        "# axarr[0].imshow(X[e,:,:,:])\n",
        "axarr[0].imshow(y_predict)\n",
        "\n",
        "# plot for actual density map\n",
        "axarr[1].title.set_text('actual density map')\n",
        "# axarr[1].imshow(X[e,:,:,:])\n",
        "axarr[1].imshow(y[e,:,:,0])\n",
        "print('Predicted Number of people: %4.2f' %(np.sum(y_predict)/1000))\n",
        "print('Exact Number of people: %4.2f' %(np.sum(y[e])/1000))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve8_BnqOhXx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB8VQZqD6-ga",
        "colab_type": "text"
      },
      "source": [
        "### Data uploaded from user"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPzw7oUd7Wv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # pre-process image\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(480, 640))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  images[0] = images[0] / 255\n",
        "\n",
        "  # predicting images\n",
        "  y_predict = full_eval(images,smooth=True)\n",
        "\n",
        "  print(\"\\nPrediction for data {}\".format(fn))\n",
        "  print('Predicted Number of people: %4.2f\\n' %(np.sum(y_predict)/1000))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}